{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1598e9b-6537-4ebb-9e28-0c41efeb0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 模型类别 & 特征配置 =====\n",
    "\n",
    "# 给网络流量项目起一个 ID\n",
    "model_cat_id = \"NF01\"   # Network Flow Model 01\n",
    "\n",
    "# 目标变量：我们要预测的是真实流量 n_flows\n",
    "target_col = \"n_flows\"\n",
    "\n",
    "# 流量统计特征（输入特征的一部分）\n",
    "network_feature_cols = [\n",
    "    'n_packets',\n",
    "    'n_bytes',\n",
    "    'n_dest_asn',\n",
    "    'n_dest_ports',\n",
    "    'n_dest_ip',\n",
    "    'tcp_udp_ratio_packets',\n",
    "    'tcp_udp_ratio_bytes',\n",
    "    'dir_ratio_packets',\n",
    "    'dir_ratio_bytes',\n",
    "    'avg_duration',\n",
    "    'avg_ttl'\n",
    "]\n",
    "\n",
    "# “日历”特征：从 id_time 构造出来，而不是用真实日期\n",
    "#   - hour_of_day: 一天中的第几小时（0~23）\n",
    "#   - day_index  : 从 0 开始的第几天\n",
    "calendar_feature_cols = ['hour_of_day', 'day_index']\n",
    "\n",
    "# LSTM 层配置（沿用你原来的网格搜索方式）\n",
    "layer_conf = [True, True, True]  # 最多 3 层 LSTM\n",
    "cells = [\n",
    "    [5, 10, 20, 30, 50, 75, 100, 125, 150],  # 第一层可能的单元数\n",
    "    [0, 10, 20, 50],                         # 第二层\n",
    "    [0, 10, 15, 20]                          # 第三层\n",
    "]\n",
    "dropout = [0, 0.1, 0.2]          # dropout 候选\n",
    "batch_size = [8]                 # 批大小（固定 8）\n",
    "timesteps = [60]                  # 单步预测（每次只看当前时刻特征）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bae846-404b-4951-8c7d-d05f57cc4377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== 模块导入 =====\n",
    "import os, sys, time as t, datetime as dt, pytz, math, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (9,5)\n",
    "\n",
    "# 把项目根目录（上一级）加入 path，继续用你原来的 lstm 模块\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from lstm_load_forecasting import lstm   # 现在只用 lstm，data 我们自己写\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f29c548-887c-4773-b553-81f25143cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 全局配置 =====\n",
    "\n",
    "# 数据文件路径：假设你把 11minutes.csv 放在 ../data/ 下\n",
    "# path = os.path.join(os.path.abspath(''), '../data/11minutes.csv')\n",
    "path = os.path.join(os.getcwd(), 'data', '11minutes.csv')\n",
    "\n",
    "# 不再用 split_date（因为没有真实 datetime），用 80%/20% 切分\n",
    "train_ratio = 0.8\n",
    "\n",
    "validation_split = 0.2   # 训练集内部再划 20% 为验证集\n",
    "epochs = 30\n",
    "verbose = 0\n",
    "\n",
    "# 结果 DataFrame\n",
    "results = pd.DataFrame(columns=[\n",
    "    'model_name', 'config', 'dropout',\n",
    "    'train_loss', 'train_rmse', 'train_mae', 'train_mape', \n",
    "    'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape', \n",
    "    'test_rmse', 'test_mae', 'test_mape',\n",
    "    'epochs', 'batch_train', 'input_shape',\n",
    "    'total_time', 'time_step', 'splits'\n",
    "])\n",
    "\n",
    "# Early Stopping 参数\n",
    "early_stopping = True\n",
    "min_delta = 0.006\n",
    "patience = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d14036-b44b-499d-8d6b-5e9b9dd8e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 输出目录 & 模型组合 =====\n",
    "\n",
    "# res_dir   = '../results/notebook_' + model_cat_id + '/'\n",
    "# plot_dir  = '../plots/notebook_'   + model_cat_id + '/'\n",
    "# model_dir = '../models/notebook_'  + model_cat_id + '/'\n",
    "\n",
    "\n",
    "# os.makedirs(res_dir,   exist_ok=True)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "# os.makedirs(plot_dir,  exist_ok=True)\n",
    "\n",
    "# output_table      = os.path.join(res_dir,  model_cat_id + '_results_'      + t.strftime(\"%Y%m%d\") + '.csv')\n",
    "# test_output_table = os.path.join(res_dir,  model_cat_id + '_test_results_' + t.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "# # 生成模型组合（和电力版本一样的方式）\n",
    "# models = lstm.generate_combinations(\n",
    "#     model_name=model_cat_id + '_',\n",
    "#     layer_conf=layer_conf,\n",
    "#     cells=cells,\n",
    "#     dropout=dropout,\n",
    "#     batch_size=batch_size,\n",
    "#     timesteps=[1]\n",
    "# )\n",
    "\n",
    "# print(\"Number of model configs generated:\", len(models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a289ac33-81e6-453b-97c4-f5c35b129e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "| Number of model configs generated | 432 |\n",
      "Number of model configs generated: 432\n"
     ]
    }
   ],
   "source": [
    "# ===== 输出目录 & 模型组合 =====\n",
    "\n",
    "# 你的项目根目录\n",
    "base_dir = r\"D:\\Downloads\\lstm-load-forecasting-master\\lstm-load-forecasting-master\"\n",
    "\n",
    "# 在根目录下建立独立的 NF01_results / models / plots\n",
    "res_dir   = os.path.join(base_dir, f\"{model_cat_id}_results/\")\n",
    "plot_dir  = os.path.join(base_dir, f\"{model_cat_id}_plots/\")\n",
    "model_dir = os.path.join(base_dir, f\"{model_cat_id}_models/\")\n",
    "\n",
    "# 确保目录存在\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# 输出结果文件名\n",
    "output_table = os.path.join(\n",
    "    res_dir, model_cat_id + \"_results_\" + t.strftime(\"%Y%m%d\") + \".csv\"\n",
    ")\n",
    "test_output_table = os.path.join(\n",
    "    res_dir, model_cat_id + \"_test_results_\" + t.strftime(\"%Y%m%d\") + \".csv\"\n",
    ")\n",
    "\n",
    "# 生成模型组合\n",
    "models = lstm.generate_combinations(\n",
    "    model_name=model_cat_id + \"_\",\n",
    "    layer_conf=layer_conf,\n",
    "    cells=cells,\n",
    "    dropout=dropout,\n",
    "    batch_size=batch_size,\n",
    "    timesteps=[60]\n",
    ")\n",
    "\n",
    "print(\"Number of model configs generated:\", len(models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b17a703-e8c0-4859-9712-9f4d2af6507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (40298, 14)\n",
      "Columns: Index(['actual', 'hour_of_day', 'day_index', 'n_packets', 'n_bytes',\n",
      "       'n_dest_asn', 'n_dest_ports', 'n_dest_ip', 'tcp_udp_ratio_packets',\n",
      "       'tcp_udp_ratio_bytes', 'dir_ratio_packets', 'dir_ratio_bytes',\n",
      "       'avg_duration', 'avg_ttl'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_index</th>\n",
       "      <th>n_packets</th>\n",
       "      <th>n_bytes</th>\n",
       "      <th>n_dest_asn</th>\n",
       "      <th>n_dest_ports</th>\n",
       "      <th>n_dest_ip</th>\n",
       "      <th>tcp_udp_ratio_packets</th>\n",
       "      <th>tcp_udp_ratio_bytes</th>\n",
       "      <th>dir_ratio_packets</th>\n",
       "      <th>dir_ratio_bytes</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>avg_ttl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31049.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76327</td>\n",
       "      <td>5806461</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>19475.0</td>\n",
       "      <td>21327.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>7.12</td>\n",
       "      <td>64.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32765.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77374</td>\n",
       "      <td>5887159</td>\n",
       "      <td>1870.0</td>\n",
       "      <td>20386.0</td>\n",
       "      <td>22550.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.11</td>\n",
       "      <td>63.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30469.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71407</td>\n",
       "      <td>5432005</td>\n",
       "      <td>1848.0</td>\n",
       "      <td>19425.0</td>\n",
       "      <td>21712.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>62.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29960.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69281</td>\n",
       "      <td>5271808</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>19161.0</td>\n",
       "      <td>21388.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.50</td>\n",
       "      <td>63.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35818.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79865</td>\n",
       "      <td>6082582</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>22111.0</td>\n",
       "      <td>25013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.00</td>\n",
       "      <td>63.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               actual  hour_of_day  day_index  n_packets  n_bytes  n_dest_asn  \\\n",
       "minute_index                                                                    \n",
       "0             31049.0            0          0      76327  5806461      1866.0   \n",
       "1             32765.0            0          0      77374  5887159      1870.0   \n",
       "2             30469.0            0          0      71407  5432005      1848.0   \n",
       "3             29960.0            0          0      69281  5271808      1890.0   \n",
       "4             35818.0            0          0      79865  6082582      1955.0   \n",
       "\n",
       "              n_dest_ports  n_dest_ip  tcp_udp_ratio_packets  \\\n",
       "minute_index                                                   \n",
       "0                  19475.0    21327.0                    0.0   \n",
       "1                  20386.0    22550.0                    0.0   \n",
       "2                  19425.0    21712.0                    0.0   \n",
       "3                  19161.0    21388.0                    0.0   \n",
       "4                  22111.0    25013.0                    0.0   \n",
       "\n",
       "              tcp_udp_ratio_bytes  dir_ratio_packets  dir_ratio_bytes  \\\n",
       "minute_index                                                            \n",
       "0                             0.0               0.49             0.49   \n",
       "1                             0.0               0.49             0.49   \n",
       "2                             0.0               0.50             0.49   \n",
       "3                             0.0               0.50             0.50   \n",
       "4                             0.0               0.49             0.49   \n",
       "\n",
       "              avg_duration  avg_ttl  \n",
       "minute_index                         \n",
       "0                     7.12    64.02  \n",
       "1                     6.11    63.16  \n",
       "2                     5.49    62.55  \n",
       "3                     6.50    63.42  \n",
       "4                     6.00    63.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 读取 11minutes.csv，并构造特征 =====\n",
    "\n",
    "df_raw = pd.read_csv(path)\n",
    "\n",
    "# 目标列 actual = n_flows（转成 float）\n",
    "df_raw['actual'] = df_raw[target_col].astype(float)\n",
    "\n",
    "# 构造“伪日历”特征：从 id_time 推出 hour_of_day & day_index\n",
    "# 这里假设 id_time 是连续的“分钟索引”（0,1,2,...）\n",
    "df_raw['minute_index'] = df_raw['id_time']\n",
    "\n",
    "minutes_per_day = 60 * 24\n",
    "df_raw['hour_of_day'] = (df_raw['minute_index'] % minutes_per_day) // 60   # 0 ~ 23\n",
    "df_raw['day_index']   = (df_raw['minute_index'] // minutes_per_day)       # 第几天，从 0 开始\n",
    "\n",
    "# 设置索引为 minute_index，时间顺序很重要\n",
    "df_raw = df_raw.set_index('minute_index').sort_index()\n",
    "\n",
    "# 我们要用的全部特征列：\n",
    "used_feature_cols = calendar_feature_cols + network_feature_cols\n",
    "\n",
    "# 只保留 actual + 特征（避免乱入没用的列）\n",
    "df = df_raw[['actual'] + used_feature_cols].copy()\n",
    "\n",
    "# 丢掉缺失（一般不会有，但保险）\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e03d42-bdd1-41de-83af-f2131f55c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (32238, 13) Test size: (8060, 13)\n"
     ]
    }
   ],
   "source": [
    "# ===== 标准化特征，并划分训练/测试集 =====\n",
    "\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# 只标准化 float 类型的列（含 actual，和你原来电力项目保持一致）\n",
    "float_cols = [c for c in df_scaled.columns if df_scaled[c].dtype == 'float64' or df_scaled[c].dtype == 'float32']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled[float_cols] = scaler.fit_transform(df_scaled[float_cols])\n",
    "\n",
    "# 按顺序切 80% 做训练，20% 做测试\n",
    "n_samples   = len(df_scaled)\n",
    "split_index = int(n_samples * train_ratio)\n",
    "\n",
    "df_train = df_scaled.iloc[:split_index].copy()\n",
    "df_test  = df_scaled.iloc[split_index:].copy()\n",
    "\n",
    "# 拆成 X, y\n",
    "y_train = df_train['actual'].copy()\n",
    "X_train = df_train.drop(columns=['actual'])\n",
    "\n",
    "y_test  = df_test['actual'].copy()\n",
    "X_test  = df_test.drop(columns=['actual'])\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d78d578-3d66-4e18-9249-9b3f67203ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 轻量版多模型训练（只训前 N 个）=====\n",
    "\n",
    "# MAX_MODELS   = 8       # 先训练前 8 个试试\n",
    "# EPOCHS       = 10      # epochs 降低一点\n",
    "# PATIENCE     = 1\n",
    "# MIN_DELTA    = 0.01\n",
    "# SAVE_MODELS  = True    # 这里改成 True，这样后面评估时能加载模型\n",
    "# FLUSH_EVERY  = 2\n",
    "\n",
    "# epochs    = EPOCHS\n",
    "# patience  = PATIENCE\n",
    "# min_delta = MIN_DELTA\n",
    "\n",
    "# models = models[:MAX_MODELS]\n",
    "# print(\"Will train\", len(models), \"models\")\n",
    "\n",
    "# # 从 history 的字典中兼容地取指标\n",
    "# def pick(h, *keys):\n",
    "#     for k in keys:\n",
    "#         if k in h:\n",
    "#             return h[k]\n",
    "#     raise KeyError(f\"history 中没有 {keys}；可用键：{list(h.keys())}\")\n",
    "\n",
    "# start_time   = t.time()\n",
    "# pending_rows = []\n",
    "\n",
    "# for idx, m in enumerate(models, 1):\n",
    "#     stopper = t.time()\n",
    "#     print('========================= Model {}/{} ========================='.format(idx, len(models)))\n",
    "#     print(tabulate(\n",
    "#         [['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]],\n",
    "#         tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "#     ))\n",
    "#     try:\n",
    "#         # 1) 构建模型\n",
    "#         model = lstm.create_model(\n",
    "#             layers=m['layers'],\n",
    "#             sample_size=X_train.shape[0],\n",
    "#             batch_size=m['batch_size'],\n",
    "#             timesteps=m['timesteps'],\n",
    "#             features=X_train.shape[1]\n",
    "#         )\n",
    "\n",
    "#         # 2) 训练模型\n",
    "#         history = lstm.train_model(\n",
    "#             model=model, mode='fit',\n",
    "#             y=y_train, X=X_train,\n",
    "#             batch_size=m['batch_size'],\n",
    "#             timesteps=m['timesteps'],\n",
    "#             epochs=epochs,\n",
    "#             rearrange=True,\n",
    "#             validation_split=validation_split,\n",
    "#             verbose=1,\n",
    "#             early_stopping=early_stopping,\n",
    "#             min_delta=min_delta,\n",
    "#             patience=patience\n",
    "#         )\n",
    "\n",
    "#         # 3) 取最优 epoch\n",
    "#         h = history.history\n",
    "#         val_loss_hist   = pick(h, 'val_loss')\n",
    "#         train_loss_hist = pick(h, 'loss')\n",
    "#         train_mae_hist  = pick(h, 'mae', 'mean_absolute_error')\n",
    "#         val_mae_hist    = pick(h, 'val_mae', 'val_mean_absolute_error')\n",
    "\n",
    "#         min_idx   = int(np.argmin(val_loss_hist))\n",
    "#         min_epoch = min_idx + 1\n",
    "\n",
    "#         # 4) 记录一行结果\n",
    "#         row = {\n",
    "#             'model_name': m['name'],\n",
    "#             'config': m,\n",
    "#             'dropout': m['layers'][0].get('dropout', 0),\n",
    "#             'train_loss': float(train_loss_hist[min_idx]),\n",
    "#             'train_rmse': 0,\n",
    "#             'train_mae' : float(train_mae_hist[min_idx]),\n",
    "#             'train_mape': 0,\n",
    "#             'valid_loss': float(val_loss_hist[min_idx]),\n",
    "#             'valid_rmse': 0,\n",
    "#             'valid_mae' : float(val_mae_hist[min_idx]),\n",
    "#             'valid_mape': 0,\n",
    "#             'test_rmse': 0, 'test_mae': 0, 'test_mape': 0,\n",
    "#             'epochs': f'{min_epoch}/{epochs}',\n",
    "#             'batch_train': m['batch_size'],\n",
    "#             'input_shape': (X_train.shape[0], m['timesteps'], X_train.shape[1]),\n",
    "#             'total_time': t.time() - stopper,\n",
    "#             'time_step': 0,\n",
    "#             'splits': f\"0-{split_index}\"\n",
    "#         }\n",
    "#         pending_rows.append(row)\n",
    "\n",
    "#         # 5) 保存模型\n",
    "#         if SAVE_MODELS:\n",
    "#             model_path = os.path.join(model_dir, f\"{m['name']}.h5\")\n",
    "#             model.save(model_path)\n",
    "\n",
    "#         # 6) 定期把结果写入 CSV\n",
    "#         if (idx % FLUSH_EVERY == 0) or (idx == len(models)):\n",
    "#             if pending_rows:\n",
    "#                 results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "#                 results.to_csv(output_table, sep=';', index=False)\n",
    "#                 pending_rows = []\n",
    "\n",
    "#     except BaseException as e:\n",
    "#         print('=============== ERROR {}/{} ============='.format(idx, len(models)))\n",
    "#         print(tabulate(\n",
    "#             [['Model:', m['name']], ['Config:', m]],\n",
    "#             tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "#         ))\n",
    "#         print('Error:', e)\n",
    "#         pending_rows.append({'model_name': m['name'], 'config': m, 'train_loss': str(e)})\n",
    "#         results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "#         results.to_csv(output_table, sep=';', index=False)\n",
    "#         pending_rows = []\n",
    "#     finally:\n",
    "#         keras.backend.clear_session()\n",
    "#         try:\n",
    "#             del model\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# # 确保 pending_rows 不丢\n",
    "# if pending_rows:\n",
    "#     results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "#     results.to_csv(output_table, sep=';', index=False)\n",
    "\n",
    "# print(\"Done. Results saved to:\", output_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595ed2bf-e7b7-40ff-a418-e4c63cf2a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train 8 models\n",
      "========================= Model 1/8 =========================\n",
      "| Starting with model | NF01_1_l-5                 |\n",
      "| Starting time       | 2025-11-18 23:00:33.077922 |\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3218/3218 [==============================] - 44s 13ms/step - loss: 0.6952 - mae: 0.6777 - val_loss: 2.7578 - val_mae: 1.3967\n",
      "Epoch 2/10\n",
      "3215/3218 [============================>.] - ETA: 0s - loss: 0.6986 - mae: 0.6792Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 42s 13ms/step - loss: 0.6982 - mae: 0.6789 - val_loss: 2.7582 - val_mae: 1.3968\n",
      "Epoch 2: early stopping\n",
      "========================= Model 2/8 =========================\n",
      "| Starting with model | NF01_2_l-5_d-0.1           |\n",
      "| Starting time       | 2025-11-18 23:02:00.238393 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 42s 12ms/step - loss: 0.7003 - mae: 0.6794 - val_loss: 2.7870 - val_mae: 1.4057\n",
      "Epoch 2/10\n",
      "3218/3218 [==============================] - ETA: 0s - loss: 0.7213 - mae: 0.6903Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 41s 13ms/step - loss: 0.7213 - mae: 0.6903 - val_loss: 2.7877 - val_mae: 1.4059\n",
      "Epoch 2: early stopping\n",
      "========================= Model 3/8 =========================\n",
      "| Starting with model | NF01_3_l-5_d-0.2           |\n",
      "| Starting time       | 2025-11-18 23:03:24.294411 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\UserData\\AppData\\Local\\Temp\\ipykernel_46156\\1268297648.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 43s 13ms/step - loss: 0.6959 - mae: 0.6782 - val_loss: 2.7211 - val_mae: 1.3854\n",
      "Epoch 2/10\n",
      "3217/3218 [============================>.] - ETA: 0s - loss: 0.7166 - mae: 0.6888Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 39s 12ms/step - loss: 0.7164 - mae: 0.6887 - val_loss: 2.7302 - val_mae: 1.3882\n",
      "Epoch 2: early stopping\n",
      "========================= Model 4/8 =========================\n",
      "| Starting with model | NF01_4_l-5_l-10            |\n",
      "| Starting time       | 2025-11-18 23:04:46.924901 |\n",
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 85s 25ms/step - loss: 0.5769 - mae: 0.6131 - val_loss: 2.7105 - val_mae: 1.3821\n",
      "Epoch 2/10\n",
      "3218/3218 [==============================] - 79s 25ms/step - loss: 0.6023 - mae: 0.6272 - val_loss: 2.6922 - val_mae: 1.3764\n",
      "Epoch 3/10\n",
      "3217/3218 [============================>.] - ETA: 0s - loss: 0.6092 - mae: 0.6312Restoring model weights from the end of the best epoch: 2.\n",
      "3218/3218 [==============================] - 79s 25ms/step - loss: 0.6091 - mae: 0.6310 - val_loss: 2.6968 - val_mae: 1.3778\n",
      "Epoch 3: early stopping\n",
      "========================= Model 5/8 =========================\n",
      "| Starting with model | NF01_5_l-5_l-10_d-0.1      |\n",
      "| Starting time       | 2025-11-18 23:08:51.016803 |\n",
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 87s 26ms/step - loss: 0.5989 - mae: 0.6248 - val_loss: 2.6436 - val_mae: 1.3613\n",
      "Epoch 2/10\n",
      "3218/3218 [==============================] - ETA: 0s - loss: 0.6115 - mae: 0.6318Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 82s 26ms/step - loss: 0.6115 - mae: 0.6318 - val_loss: 2.6692 - val_mae: 1.3693\n",
      "Epoch 2: early stopping\n",
      "========================= Model 6/8 =========================\n",
      "| Starting with model | NF01_6_l-5_l-10_d-0.2      |\n",
      "| Starting time       | 2025-11-18 23:11:41.224704 |\n",
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 85s 25ms/step - loss: 0.6090 - mae: 0.6309 - val_loss: 2.6305 - val_mae: 1.3572\n",
      "Epoch 2/10\n",
      "3216/3218 [============================>.] - ETA: 0s - loss: 0.6181 - mae: 0.6362Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 79s 25ms/step - loss: 0.6178 - mae: 0.6360 - val_loss: 2.6413 - val_mae: 1.3606\n",
      "Epoch 2: early stopping\n",
      "========================= Model 7/8 =========================\n",
      "| Starting with model | NF01_7_l-5_l-15            |\n",
      "| Starting time       | 2025-11-18 23:14:26.380036 |\n",
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 82s 24ms/step - loss: 0.5707 - mae: 0.6098 - val_loss: 2.7108 - val_mae: 1.3822\n",
      "Epoch 2/10\n",
      "3218/3218 [==============================] - 79s 25ms/step - loss: 0.5923 - mae: 0.6222 - val_loss: 2.6993 - val_mae: 1.3786\n",
      "Epoch 3/10\n",
      "3218/3218 [==============================] - 80s 25ms/step - loss: 0.5954 - mae: 0.6241 - val_loss: 2.6853 - val_mae: 1.3742\n",
      "Epoch 4/10\n",
      "3217/3218 [============================>.] - ETA: 0s - loss: 0.6002 - mae: 0.6264Restoring model weights from the end of the best epoch: 3.\n",
      "3218/3218 [==============================] - 80s 25ms/step - loss: 0.6000 - mae: 0.6263 - val_loss: 2.6854 - val_mae: 1.3743\n",
      "Epoch 4: early stopping\n",
      "========================= Model 8/8 =========================\n",
      "| Starting with model | NF01_8_l-5_l-15_d-0.1      |\n",
      "| Starting time       | 2025-11-18 23:19:47.734860 |\n",
      "Epoch 1/10\n",
      "3218/3218 [==============================] - 85s 25ms/step - loss: 0.5855 - mae: 0.6181 - val_loss: 2.6680 - val_mae: 1.3689\n",
      "Epoch 2/10\n",
      "3216/3218 [============================>.] - ETA: 0s - loss: 0.6085 - mae: 0.6300Restoring model weights from the end of the best epoch: 1.\n",
      "3218/3218 [==============================] - 83s 26ms/step - loss: 0.6082 - mae: 0.6298 - val_loss: 2.6640 - val_mae: 1.3676\n",
      "Epoch 2: early stopping\n",
      "Done. Results saved to: D:\\Downloads\\lstm-load-forecasting-master\\lstm-load-forecasting-master\\NF01_results/NF01_results_20251118.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== 轻量版多模型训练（只训前 N 个）=====\n",
    "\n",
    "MAX_MODELS   = 8       # 先训练前 8 个试试\n",
    "EPOCHS       = 10      # epochs 降低一点\n",
    "PATIENCE     = 1\n",
    "MIN_DELTA    = 0.01\n",
    "SAVE_MODELS  = True    # 保存每个模型，后面要评估\n",
    "FLUSH_EVERY  = 2       # 每隔多少个模型，把结果落盘一次\n",
    "\n",
    "epochs    = EPOCHS\n",
    "patience  = PATIENCE\n",
    "min_delta = MIN_DELTA\n",
    "\n",
    "models = models[:MAX_MODELS]\n",
    "print(\"Will train\", len(models), \"models\")\n",
    "\n",
    "# 从 history 的字典中兼容地取指标\n",
    "def pick(h, *keys):\n",
    "    for k in keys:\n",
    "        if k in h:\n",
    "            return h[k]\n",
    "    raise KeyError(f\"history 中没有 {keys}；可用键：{list(h.keys())}\")\n",
    "\n",
    "start_time   = t.time()\n",
    "pending_rows = []\n",
    "\n",
    "for idx, m in enumerate(models, 1):\n",
    "    stopper = t.time()\n",
    "    print('========================= Model {}/{} ========================='.format(idx, len(models)))\n",
    "    print(tabulate(\n",
    "        [['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]],\n",
    "        tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "    ))\n",
    "    try:\n",
    "        # 1) 构建模型 —— 注意这里删掉 sample_size 和 batch_size 参数\n",
    "        model = lstm.create_model(\n",
    "            layers=m['layers'],\n",
    "            timesteps=m['timesteps'],\n",
    "            features=X_train.shape[1]\n",
    "        )\n",
    "\n",
    "        # 2) 训练模型 —— rearrange=True 让它用时间窗口\n",
    "        history = lstm.train_model(\n",
    "            model=model, mode='fit',\n",
    "            y=y_train, X=X_train,\n",
    "            batch_size=m['batch_size'],\n",
    "            timesteps=m['timesteps'],\n",
    "            epochs=epochs,\n",
    "            rearrange=True,                 # ★ 用滑动窗口\n",
    "            validation_split=validation_split,\n",
    "            verbose=1,\n",
    "            early_stopping=early_stopping,\n",
    "            min_delta=min_delta,\n",
    "            patience=patience\n",
    "        )\n",
    "\n",
    "        # 3) 取最优 epoch\n",
    "        h = history.history\n",
    "        val_loss_hist   = pick(h, 'val_loss')\n",
    "        train_loss_hist = pick(h, 'loss')\n",
    "        train_mae_hist  = pick(h, 'mae', 'mean_absolute_error')\n",
    "        val_mae_hist    = pick(h, 'val_mae', 'val_mean_absolute_error')\n",
    "\n",
    "        min_idx   = int(np.argmin(val_loss_hist))\n",
    "        min_epoch = min_idx + 1\n",
    "\n",
    "        # 4) 记录一行结果（先放在 pending_rows）\n",
    "        row = {\n",
    "            'model_name': m['name'],\n",
    "            'config': m,\n",
    "            'dropout': m['layers'][0].get('dropout', 0),\n",
    "            'train_loss': float(train_loss_hist[min_idx]),\n",
    "            'train_rmse': 0,\n",
    "            'train_mae' : float(train_mae_hist[min_idx]),\n",
    "            'train_mape': 0,\n",
    "            'valid_loss': float(val_loss_hist[min_idx]),\n",
    "            'valid_rmse': 0,\n",
    "            'valid_mae' : float(val_mae_hist[min_idx]),\n",
    "            'valid_mape': 0,\n",
    "            'test_rmse': 0, 'test_mae': 0, 'test_mape': 0,\n",
    "            'epochs': f'{min_epoch}/{epochs}',\n",
    "            'batch_train': m['batch_size'],\n",
    "            'input_shape': (m['timesteps'], X_train.shape[1]),\n",
    "            'total_time': t.time() - stopper,\n",
    "            'time_step': m['timesteps'],\n",
    "            'splits': f\"0-{len(X_train)}\"\n",
    "        }\n",
    "        pending_rows.append(row)\n",
    "\n",
    "        # 5) 保存模型\n",
    "        if SAVE_MODELS:\n",
    "            model_path = os.path.join(model_dir, f\"{m['name']}.h5\")\n",
    "            model.save(model_path)\n",
    "\n",
    "        # 6) 每隔 FLUSH_EVERY 个模型，把结果写入 CSV 一次\n",
    "        if (idx % FLUSH_EVERY == 0) or (idx == len(models)):\n",
    "            if pending_rows:\n",
    "                results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "                # 这里直接覆盖写，不用 append\n",
    "                results.to_csv(output_table, sep=';', index=False)\n",
    "                pending_rows = []\n",
    "\n",
    "    except BaseException as e:\n",
    "        print('=============== ERROR {}/{} ============='.format(idx, len(models)))\n",
    "        print(tabulate(\n",
    "            [['Model:', m['name']], ['Config:', m]],\n",
    "            tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "        ))\n",
    "        print('Error:', e)\n",
    "        # 把错误也记一条进去，不在这里写文件\n",
    "        pending_rows.append({'model_name': m['name'], 'config': m, 'train_loss': str(e)})\n",
    "    finally:\n",
    "        try:\n",
    "            keras.backend.clear_session()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 循环结束后，确保 pending_rows 不丢\n",
    "if pending_rows:\n",
    "    results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "    results.to_csv(output_table, sep=';', index=False)\n",
    "\n",
    "print(\"Done. Results saved to:\", output_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56dee33-7eae-49be-84fb-e0b7a759e699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5a724d-1810-4394-9986-0b5929dc5cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>valid_mae</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NF01_6_l-5_l-10_d-0.2</td>\n",
       "      <td>1.357150</td>\n",
       "      <td>0.630870</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF01_5_l-5_l-10_d-0.1</td>\n",
       "      <td>1.361250</td>\n",
       "      <td>0.624810</td>\n",
       "      <td>1/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NF01_8_l-5_l-15_d-0.1</td>\n",
       "      <td>1.367617</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>2/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NF01_7_l-5_l-15</td>\n",
       "      <td>1.374235</td>\n",
       "      <td>0.624098</td>\n",
       "      <td>3/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF01_4_l-5_l-10</td>\n",
       "      <td>1.376397</td>\n",
       "      <td>0.627177</td>\n",
       "      <td>2/10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_name  valid_mae  train_mae epochs\n",
       "5  NF01_6_l-5_l-10_d-0.2   1.357150   0.630870   1/10\n",
       "4  NF01_5_l-5_l-10_d-0.1   1.361250   0.624810   1/10\n",
       "7  NF01_8_l-5_l-15_d-0.1   1.367617   0.629839   2/10\n",
       "6        NF01_7_l-5_l-15   1.374235   0.624098   3/10\n",
       "3        NF01_4_l-5_l-10   1.376397   0.627177   2/10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 模型选择：按验证集 MAE 选 Top-5 =====\n",
    "import glob\n",
    "\n",
    "pattern    = os.path.join(res_dir, f\"{model_cat_id}_results_*.csv\")\n",
    "candidates = sorted(glob.glob(pattern))\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    results_csv = results.copy()\n",
    "else:\n",
    "    results_fn  = candidates[-1]  # 最新的一个\n",
    "    results_csv = pd.read_csv(results_fn, delimiter=';')\n",
    "\n",
    "if 'valid_mae' not in results_csv.columns:\n",
    "    raise ValueError(f\"'valid_mae' 列不存在。可用列：{list(results_csv.columns)}\")\n",
    "\n",
    "top_models = results_csv.dropna(subset=['valid_mae']).nsmallest(5, 'valid_mae')\n",
    "display(top_models[['model_name', 'valid_mae', 'train_mae', 'epochs']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e404d50-e43b-4822-abf4-e2bc151d8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+-----------------------+\n",
      "| Model name            |   Mean squared error |   Mean absolute error |\n",
      "+=======================+======================+=======================+\n",
      "| NF01_4_l-5_l-10       |               0.4946 |                0.5544 |\n",
      "+-----------------------+----------------------+-----------------------+\n",
      "| NF01_7_l-5_l-15       |               0.4953 |                0.5549 |\n",
      "+-----------------------+----------------------+-----------------------+\n",
      "| NF01_8_l-5_l-15_d-0.1 |               0.4973 |                0.5561 |\n",
      "+-----------------------+----------------------+-----------------------+\n",
      "| NF01_5_l-5_l-10_d-0.1 |               0.5003 |                0.5580 |\n",
      "+-----------------------+----------------------+-----------------------+\n",
      "| NF01_6_l-5_l-10_d-0.2 |               0.5020 |                0.5590 |\n",
      "+-----------------------+----------------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# ===== 在测试集上评估 Top-5 模型 =====\n",
    "\n",
    "test_rows   = []\n",
    "predictions = {}\n",
    "\n",
    "for _, row in top_models.iterrows():\n",
    "    base = os.path.join(model_dir, row['model_name'])\n",
    "    filename = base + '.h5'\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"[WARN] 模型文件不存在：{filename}，跳过\")\n",
    "        continue\n",
    "\n",
    "    model = load_model(filename)\n",
    "    batch_size_eval = int(row['batch_train']) if 'batch_train' in row and not pd.isna(row['batch_train']) else 8\n",
    "\n",
    "    loss, mae = lstm.evaluate_model(\n",
    "        model=model,\n",
    "        X=X_test,\n",
    "        y=y_test,\n",
    "        batch_size=batch_size_eval,\n",
    "        timesteps=60,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_rows.append({\n",
    "        'Model name': row['model_name'],\n",
    "        'Mean squared error': float(loss),\n",
    "        'Mean absolute error': float(mae),\n",
    "    })\n",
    "\n",
    "    # 预测（可选）\n",
    "    try:\n",
    "        model.reset_states()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    yhat = lstm.get_predictions(\n",
    "        model=model,\n",
    "        X=X_test,\n",
    "        batch_size=batch_size_eval,\n",
    "        timesteps=int(timesteps[0]),\n",
    "        verbose=0\n",
    "    )\n",
    "    predictions[row['model_name']] = np.array(yhat).reshape(-1)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    del model\n",
    "\n",
    "test_results = pd.DataFrame(test_rows)\n",
    "if test_results.empty:\n",
    "    raise RuntimeError(\"没有任何模型评估成功，请检查模型保存路径/文件名。\")\n",
    "\n",
    "test_results = test_results.sort_values('Mean absolute error', ascending=True)\n",
    "test_results = test_results.set_index(['Model name'])\n",
    "\n",
    "if not os.path.isfile(test_output_table):\n",
    "    test_results.to_csv(test_output_table, sep=';')\n",
    "else:\n",
    "    test_results.to_csv(test_output_table, mode='a', header=False, sep=';')\n",
    "\n",
    "print(tabulate(test_results, headers='keys', tablefmt='grid', numalign='right', floatfmt='.4f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f3d825d-95da-46a2-a89d-3e7ea6062864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after sequencing and batching:\n",
      "  X_eval: (8000, 60, 13)\n",
      "  y_eval: (8000,)\n",
      "Aligned lengths: 8000 8000\n",
      "MAE: 0.5590 | MSE: 0.5020\n",
      "Saved: D:\\Downloads\\lstm-load-forecasting-master\\lstm-load-forecasting-master\\NF01_results/NF01_6_l-5_l-10_d-0.2_pred_vs_actual_20251119.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "best_model_name = top_models.iloc[0]['model_name']   # 假设已选好最优模型\n",
    "best_model_file = os.path.join(model_dir, best_model_name + '.h5')\n",
    "best_model = load_model(best_model_file)\n",
    "\n",
    "batch_size = int(top_models.iloc[0].get('batch_train', 8))\n",
    "t_steps    = int(top_models.iloc[0].get('time_step', timesteps[0]))\n",
    "\n",
    "# 1. 先对测试集构造序列 —— X_seq, y_seq\n",
    "X_seq, y_seq = lstm.make_sequences(X_test, y_test, t_steps)\n",
    "\n",
    "# 2. 对齐 batch_size（evaluate_model / get_predictions 内部也是这么干的）\n",
    "max_batch = len(X_seq) // batch_size\n",
    "X_eval = X_seq[:max_batch * batch_size]\n",
    "y_eval = y_seq[:max_batch * batch_size]\n",
    "\n",
    "print(\"Shapes after sequencing and batching:\")\n",
    "print(\"  X_eval:\", X_eval.shape)   # (N, t_steps, n_features)\n",
    "print(\"  y_eval:\", y_eval.shape)   # (N,)\n",
    "\n",
    "# 3. 用同样的 X_eval 做预测（不要再让 get_predictions 重新 make_sequences 了）\n",
    "pred = best_model.predict(X_eval, batch_size=batch_size, verbose=0)\n",
    "pred = pred.reshape(-1)\n",
    "y_eval = y_eval.reshape(-1)\n",
    "\n",
    "print(\"Aligned lengths:\", len(pred), len(y_eval))\n",
    "\n",
    "# 4. 计算 MAE / MSE（现在长度肯定一致）\n",
    "test_mae = mean_absolute_error(y_eval, pred)\n",
    "test_mse = mean_squared_error(y_eval, pred)\n",
    "print(f\"MAE: {test_mae:.4f} | MSE: {test_mse:.4f}\")\n",
    "\n",
    "# 5. 如果你想保存 “预测 vs 实际”\n",
    "pred_df = pd.DataFrame({\n",
    "    \"Predictions\": pred,\n",
    "    \"Actual\": y_eval\n",
    "}, index=X_test.index[t_steps:t_steps + len(y_eval)])  # 粗略对齐一下索引\n",
    "\n",
    "pred_path = os.path.join(res_dir, f\"{best_model_name}_pred_vs_actual_{t.strftime('%Y%m%d')}.csv\")\n",
    "pred_df.to_csv(pred_path, sep=';', index=True)\n",
    "print(\"Saved:\", pred_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381f85e7-edbf-422a-b680-ae44a4ca459c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m baseline_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(y_test\u001b[38;5;241m.\u001b[39mvalues, fill_value\u001b[38;5;241m=\u001b[39my_train_mean, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m----> 6\u001b[0m mae_model    \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, np\u001b[38;5;241m.\u001b[39marray(\u001b[43mpredictions_best\u001b[49m))  \u001b[38;5;66;03m# 你的 LSTM 预测\u001b[39;00m\n\u001b[0;32m      7\u001b[0m mae_baseline \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, baseline_pred)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM MAE (scaled):    \u001b[39m\u001b[38;5;124m\"\u001b[39m, mae_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions_best' is not defined"
     ]
    }
   ],
   "source": [
    "# 1) baseline：用训练集均值来预测所有测试点\n",
    "y_train_mean = y_train.mean()\n",
    "baseline_pred = np.full_like(y_test.values, fill_value=y_train_mean, dtype=float)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_model    = mean_absolute_error(y_test, np.array(predictions_best))  # 你的 LSTM 预测\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_pred)\n",
    "\n",
    "print(\"LSTM MAE (scaled):    \", mae_model)\n",
    "print(\"Baseline MAE (scaled):\", mae_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8516009-30e7-47ab-bcd9-b836f94db64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fca8a-d206-460f-b67f-36ec16e7287c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1371fc-d7f7-439d-bd8d-427819773510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c21be3-2da6-4507-804d-eef4861a0348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (network_proj_new)",
   "language": "python",
   "name": "network_proj_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
