{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598c621e-ffdd-45b2-9190-2a83d8b806d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import time as t\n",
    "# import datetime as dt\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# # 项目内自定义模块\n",
    "# base_dir = r\"D:\\Downloads\\lstm-load-forecasting-master\\lstm-load-forecasting-master\"\n",
    "# sys.path.append(os.path.join(base_dir))                # 确保能 import lstm_load_forecasting\n",
    "# from lstm_load_forecasting import lstm                 # 你刚改好的 lstm.py\n",
    "\n",
    "# print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "# # ========= 模型类别 & 特征 =========\n",
    "# model_cat_id = \"NF01\"\n",
    "\n",
    "# # 这里用到的数据列：target = actual，其它为输入特征\n",
    "# feature_cols = [\n",
    "#     'hour_of_day', 'day_index',\n",
    "#     'n_packets', 'n_bytes',\n",
    "#     'n_dest_asn', 'n_dest_ports', 'n_dest_ip',\n",
    "#     'tcp_udp_ratio_packets', 'tcp_udp_ratio_bytes',\n",
    "#     'dir_ratio_packets', 'dir_ratio_bytes',\n",
    "#     'avg_duration', 'avg_ttl'\n",
    "# ]\n",
    "# target_col = 'actual'\n",
    "\n",
    "# # ========= 时间窗口 & 超参数搜索空间 =========\n",
    "# TIMESTEPS  = 60            # 用过去 60 分钟的序列预测下一分钟\n",
    "# timesteps  = [TIMESTEPS]\n",
    "\n",
    "# layer_conf = [True, True]  # 两层 LSTM 都启用\n",
    "# cells      = [[32, 64]]    # 第一层 32 单元，第二层 64 单元\n",
    "# dropout    = [0.0, 0.1]    # 两种 dropout 配置\n",
    "# batch_size = [64]          # 批大小（影响训练/评估）\n",
    "# early_stopping = True\n",
    "# validation_split = 0.2\n",
    "\n",
    "# EPOCHS     = 20\n",
    "# MIN_DELTA  = 0.002\n",
    "# PATIENCE   = 3\n",
    "\n",
    "# # 结果汇总 DataFrame\n",
    "# results = pd.DataFrame(columns=[\n",
    "#     'model_name', 'config', 'dropout',\n",
    "#     'train_loss', 'train_rmse', 'train_mae', 'train_mape',\n",
    "#     'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape',\n",
    "#     'test_rmse',  'test_mae',  'test_mape',\n",
    "#     'epochs', 'batch_train', 'input_shape',\n",
    "#     'total_time', 'time_step', 'splits'\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c7e68d-8335-42cb-a2a6-a1e84f6afc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 项目内自定义模块\n",
    "base_dir = r\"D:\\Downloads\\lstm-load-forecasting-master\\lstm-load-forecasting-master\"\n",
    "sys.path.append(os.path.join(base_dir))                \n",
    "from lstm_load_forecasting import lstm\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "# ========= 模型类别 & 特征 =========\n",
    "model_cat_id = \"NF01\"\n",
    "\n",
    "feature_cols = [\n",
    "    'hour_of_day', 'day_index',\n",
    "    'n_packets', 'n_bytes',\n",
    "    'n_dest_asn', 'n_dest_ports', 'n_dest_ip',\n",
    "    'tcp_udp_ratio_packets', 'tcp_udp_ratio_bytes',\n",
    "    'dir_ratio_packets', 'dir_ratio_bytes',\n",
    "    'avg_duration', 'avg_ttl'\n",
    "]\n",
    "target_col = 'actual'\n",
    "\n",
    "# ========= ⚠️ 强制修正配置 (目标：40个) ⚠️ =========\n",
    "TIMESTEPS  = 60            \n",
    "# 1. 时间步长 (4种)\n",
    "timesteps_list  = [30, 60, 90, 120] \n",
    "\n",
    "# 2. LSTM 层结构 (关键修改：用双层列表包裹，确保只算作1种结构)\n",
    "# 之前的 [True, True] 可能被误读为 \"Option1: True, Option2: True\" 导致翻倍\n",
    "layer_conf_list = [[True, True]] \n",
    "\n",
    "# 3. LSTM 单元配置 (5种)\n",
    "cells_list      = [\n",
    "    [32, 32],      \n",
    "    [64, 32],      \n",
    "    [64, 64],      \n",
    "    [128, 64],     \n",
    "    [128, 128]     \n",
    "]\n",
    "\n",
    "# 4. Dropout 配置 (2种)\n",
    "dropout_list    = [0.0, 0.2]\n",
    "\n",
    "# 5. 批大小 (1种)\n",
    "batch_size_list = [64]\n",
    "\n",
    "# ========= 训练控制参数 =========\n",
    "early_stopping   = True\n",
    "validation_split = 0.2   \n",
    "\n",
    "EPOCHS     = 20\n",
    "MIN_DELTA  = 0.002\n",
    "PATIENCE   = 3\n",
    "\n",
    "# 结果汇总 DataFrame\n",
    "results = pd.DataFrame(columns=[\n",
    "    'model_name', 'config', 'dropout',\n",
    "    'train_loss', 'train_rmse', 'train_mae', 'train_mape',\n",
    "    'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape',\n",
    "    'test_rmse',  'test_mae',  'test_mape',\n",
    "    'epochs', 'batch_train', 'input_shape',\n",
    "    'total_time', 'time_step', 'splits'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4adb264-9e13-4c70-a230-cb8e565d7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (40298, 13)\n",
      "Columns: Index(['id_time', 'n_flows', 'n_packets', 'n_bytes', 'n_dest_asn',\n",
      "       'n_dest_ports', 'n_dest_ip', 'tcp_udp_ratio_packets',\n",
      "       'tcp_udp_ratio_bytes', 'dir_ratio_packets', 'dir_ratio_bytes',\n",
      "       'avg_duration', 'avg_ttl'],\n",
      "      dtype='object')\n",
      "Train size: 32238 Test size: 8060\n",
      "After scaling: X_train (32238, 11) y_train (32238,)\n"
     ]
    }
   ],
   "source": [
    "# ===== 读取原始数据 & 构造特征 =====\n",
    "data_path = os.path.join(base_dir, \"data\", \"11minutes.csv\")\n",
    "\n",
    "df_raw = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(\"Columns:\", df_raw.columns)\n",
    "\n",
    "# 目标列（你的是 n_flows）\n",
    "target_col = 'n_flows'\n",
    "df_raw['actual'] = df_raw[target_col].astype(float)\n",
    "\n",
    "# 特征列（你的真实特征）\n",
    "feature_cols = [\n",
    "    'n_packets', 'n_bytes',\n",
    "    'n_dest_asn', 'n_dest_ports', 'n_dest_ip',\n",
    "    'tcp_udp_ratio_packets', 'tcp_udp_ratio_bytes',\n",
    "    'dir_ratio_packets', 'dir_ratio_bytes',\n",
    "    'avg_duration', 'avg_ttl'\n",
    "]\n",
    "\n",
    "X_full = df_raw[feature_cols].copy()\n",
    "y_full = df_raw['actual'].copy()\n",
    "\n",
    "# ===== 按时间顺序划分 train/test（比如前 80% 做训练）=====\n",
    "n_total = len(df_raw)\n",
    "split_idx = int(n_total * 0.8)\n",
    "X_train_raw, X_test_raw = X_full.iloc[:split_idx], X_full.iloc[split_idx:]\n",
    "y_train_raw, y_test_raw = y_full.iloc[:split_idx], y_full.iloc[split_idx:]\n",
    "\n",
    "print(\"Train size:\", len(X_train_raw), \"Test size:\", len(X_test_raw))\n",
    "\n",
    "# ===== 分别对 X 和 y 做标准化（方便以后单独反归一化）=====\n",
    "scaler_X = StandardScaler()\n",
    "X_train = pd.DataFrame(\n",
    "    scaler_X.fit_transform(X_train_raw),\n",
    "    index=X_train_raw.index,\n",
    "    columns=X_train_raw.columns\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    scaler_X.transform(X_test_raw),\n",
    "    index=X_test_raw.index,\n",
    "    columns=X_test_raw.columns\n",
    ")\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = pd.Series(\n",
    "    scaler_y.fit_transform(y_train_raw.values.reshape(-1,1)).reshape(-1),\n",
    "    index=y_train_raw.index,\n",
    "    name='actual'\n",
    ")\n",
    "y_test = pd.Series(\n",
    "    scaler_y.transform(y_test_raw.values.reshape(-1,1)).reshape(-1),\n",
    "    index=y_test_raw.index,\n",
    "    name='actual'\n",
    ")\n",
    "\n",
    "print(\"After scaling: X_train\", X_train.shape, \"y_train\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677528c9-823c-4233-86b1-c6c2b785b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "| Number of model configs generated | 256 |\n",
      "检测到 256 个组合，正在强制截断为 40 个...\n",
      "Number of model configs generated: 40\n"
     ]
    }
   ],
   "source": [
    "# ===== 输出目录 & 模型组合 =====\n",
    "res_dir   = os.path.join(base_dir, f\"{model_cat_id}_results/\")\n",
    "plot_dir  = os.path.join(base_dir, f\"{model_cat_id}_plots/\")\n",
    "model_dir = os.path.join(base_dir, f\"{model_cat_id}_models/\")\n",
    "\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "output_table = os.path.join(\n",
    "    res_dir, model_cat_id + \"_results_\" + t.strftime(\"%Y%m%d\") + \".csv\"\n",
    ")\n",
    "test_output_table = os.path.join(\n",
    "    res_dir, model_cat_id + \"_test_results_\" + t.strftime(\"%Y%m%d\") + \".csv\"\n",
    ")\n",
    "\n",
    "# # 生成模型组合\n",
    "# models = lstm.generate_combinations(\n",
    "#     model_name=model_cat_id + \"_\",\n",
    "#     layer_conf=layer_conf,\n",
    "#     cells=cells,\n",
    "#     dropout=dropout,\n",
    "#     batch_size=batch_size,\n",
    "#     timesteps=timesteps\n",
    "# )\n",
    "\n",
    "# print(\"Number of model configs generated:\", len(models))\n",
    "\n",
    "# 生成模型组合\n",
    "models = lstm.generate_combinations(\n",
    "    model_name=model_cat_id + \"_\",\n",
    "    layer_conf=layer_conf_list,  # 使用新变量名\n",
    "    cells=cells_list,            # 使用新变量名\n",
    "    dropout=dropout_list,        # 使用新变量名\n",
    "    batch_size=batch_size_list,  # 使用新变量名\n",
    "    timesteps=timesteps_list     # 使用新变量名\n",
    ")\n",
    "\n",
    "# ⚠️ 强制截断：无论生成多少，只取前 40 个\n",
    "# 这能防止 256 个模型把电脑跑死\n",
    "if len(models) > 40:\n",
    "    print(f\"检测到 {len(models)} 个组合，正在强制截断为 40 个...\")\n",
    "    models = models[:40]\n",
    "\n",
    "print(\"Number of model configs generated:\", len(models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3909caa-69f1-433d-9a8c-4f168296120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 1/40 =========================\n",
      "| Starting with model | NF01_1_l-32_l-64_l-64_l-128_l-128 |\n",
      "| Starting time       | 2025-12-03 18:15:05.876807        |\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "403/403 [==============================] - 47s 96ms/step - loss: 0.2632 - mae: 0.3836 - val_loss: 0.7873 - val_mae: 0.7366\n",
      "Epoch 2/20\n",
      "403/403 [==============================] - 41s 101ms/step - loss: 0.1508 - mae: 0.2791 - val_loss: 0.4559 - val_mae: 0.5157\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 41s 101ms/step - loss: 0.1170 - mae: 0.2433 - val_loss: 0.3428 - val_mae: 0.4384\n",
      "Epoch 4/20\n",
      "403/403 [==============================] - 41s 103ms/step - loss: 0.0968 - mae: 0.2220 - val_loss: 0.3066 - val_mae: 0.4118\n",
      "Epoch 5/20\n",
      "403/403 [==============================] - 43s 106ms/step - loss: 0.0891 - mae: 0.2133 - val_loss: 0.3140 - val_mae: 0.4088\n",
      "Epoch 6/20\n",
      "403/403 [==============================] - 41s 101ms/step - loss: 0.0887 - mae: 0.2136 - val_loss: 0.2832 - val_mae: 0.3832\n",
      "Epoch 7/20\n",
      "403/403 [==============================] - 35s 88ms/step - loss: 0.0881 - mae: 0.2130 - val_loss: 0.2741 - val_mae: 0.3758\n",
      "Epoch 8/20\n",
      "403/403 [==============================] - 36s 89ms/step - loss: 0.0873 - mae: 0.2120 - val_loss: 0.2922 - val_mae: 0.3883\n",
      "Epoch 9/20\n",
      "403/403 [==============================] - 40s 98ms/step - loss: 0.0868 - mae: 0.2117 - val_loss: 0.3142 - val_mae: 0.4036\n",
      "Epoch 10/20\n",
      "403/403 [==============================] - ETA: 0s - loss: 0.0869 - mae: 0.2119Restoring model weights from the end of the best epoch: 7.\n",
      "403/403 [==============================] - 35s 87ms/step - loss: 0.0869 - mae: 0.2119 - val_loss: 0.3708 - val_mae: 0.4409\n",
      "Epoch 10: early stopping\n",
      "========================= Model 2/40 =========================\n",
      "| Starting with model | NF01_2_l-32_l-64_l-64_l-128_l-128 |\n",
      "| Starting time       | 2025-12-03 18:21:46.958149        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\network_proj_new\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "403/403 [==============================] - 76s 170ms/step - loss: 0.2542 - mae: 0.3758 - val_loss: 0.4383 - val_mae: 0.5089\n",
      "Epoch 2/20\n",
      "403/403 [==============================] - 68s 168ms/step - loss: 0.1506 - mae: 0.2797 - val_loss: 0.3007 - val_mae: 0.4119\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 65s 162ms/step - loss: 0.1177 - mae: 0.2453 - val_loss: 0.2584 - val_mae: 0.3819\n",
      "Epoch 4/20\n",
      "403/403 [==============================] - 65s 162ms/step - loss: 0.0971 - mae: 0.2223 - val_loss: 0.2573 - val_mae: 0.3850\n",
      "Epoch 5/20\n",
      "403/403 [==============================] - 67s 166ms/step - loss: 0.0894 - mae: 0.2139 - val_loss: 0.2501 - val_mae: 0.3735\n",
      "Epoch 6/20\n",
      "403/403 [==============================] - 72s 179ms/step - loss: 0.0881 - mae: 0.2126 - val_loss: 0.2474 - val_mae: 0.3696\n",
      "Epoch 7/20\n",
      "403/403 [==============================] - 72s 178ms/step - loss: 0.0874 - mae: 0.2117 - val_loss: 0.2548 - val_mae: 0.3754\n",
      "Epoch 8/20\n",
      "403/403 [==============================] - 69s 172ms/step - loss: 0.0873 - mae: 0.2118 - val_loss: 0.2762 - val_mae: 0.3926\n",
      "Epoch 9/20\n",
      "403/403 [==============================] - ETA: 0s - loss: 0.0878 - mae: 0.2129Restoring model weights from the end of the best epoch: 6.\n",
      "403/403 [==============================] - 69s 172ms/step - loss: 0.0878 - mae: 0.2129 - val_loss: 0.3162 - val_mae: 0.4238\n",
      "Epoch 9: early stopping\n",
      "========================= Model 3/40 =========================\n",
      "| Starting with model | NF01_3_l-32_l-64_l-64_l-128_l-128 |\n",
      "| Starting time       | 2025-12-03 18:32:12.035724        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\UserData\\AppData\\Local\\Temp\\ipykernel_21460\\1135675434.py:94: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "402/402 [==============================] - 119s 278ms/step - loss: 0.2566 - mae: 0.3776 - val_loss: 0.6377 - val_mae: 0.6629\n",
      "Epoch 2/20\n",
      "402/402 [==============================] - 112s 279ms/step - loss: 0.1531 - mae: 0.2824 - val_loss: 0.4716 - val_mae: 0.5356\n",
      "Epoch 3/20\n",
      "402/402 [==============================] - 111s 275ms/step - loss: 0.1200 - mae: 0.2460 - val_loss: 0.3942 - val_mae: 0.4891\n",
      "Epoch 4/20\n",
      "402/402 [==============================] - 100s 248ms/step - loss: 0.1004 - mae: 0.2262 - val_loss: 0.3602 - val_mae: 0.4686\n",
      "Epoch 5/20\n",
      "402/402 [==============================] - 110s 273ms/step - loss: 0.0908 - mae: 0.2155 - val_loss: 0.3345 - val_mae: 0.4424\n",
      "Epoch 6/20\n",
      "402/402 [==============================] - 110s 275ms/step - loss: 0.0887 - mae: 0.2135 - val_loss: 0.3137 - val_mae: 0.4233\n",
      "Epoch 7/20\n",
      "402/402 [==============================] - 109s 270ms/step - loss: 0.0884 - mae: 0.2132 - val_loss: 0.2997 - val_mae: 0.4133\n",
      "Epoch 8/20\n",
      "402/402 [==============================] - 112s 278ms/step - loss: 0.0886 - mae: 0.2138 - val_loss: 0.2903 - val_mae: 0.4089\n",
      "Epoch 9/20\n",
      "402/402 [==============================] - 113s 282ms/step - loss: 0.0877 - mae: 0.2125 - val_loss: 0.2821 - val_mae: 0.4055\n",
      "Epoch 10/20\n",
      "402/402 [==============================] - 105s 261ms/step - loss: 0.0869 - mae: 0.2113 - val_loss: 0.2789 - val_mae: 0.4022\n",
      "Epoch 11/20\n",
      "402/402 [==============================] - 104s 259ms/step - loss: 0.0867 - mae: 0.2112 - val_loss: 0.2706 - val_mae: 0.3971\n",
      "Epoch 12/20\n",
      "402/402 [==============================] - 111s 275ms/step - loss: 0.0860 - mae: 0.2104 - val_loss: 0.2762 - val_mae: 0.4004\n",
      "Epoch 13/20\n",
      "402/402 [==============================] - 112s 279ms/step - loss: 0.0855 - mae: 0.2097 - val_loss: 0.2724 - val_mae: 0.3974\n",
      "Epoch 14/20\n",
      "402/402 [==============================] - 110s 273ms/step - loss: 0.0850 - mae: 0.2090 - val_loss: 0.2585 - val_mae: 0.3846\n",
      "Epoch 15/20\n",
      "402/402 [==============================] - 110s 274ms/step - loss: 0.0841 - mae: 0.2078 - val_loss: 0.2500 - val_mae: 0.3783\n",
      "Epoch 16/20\n",
      "402/402 [==============================] - 109s 270ms/step - loss: 0.0841 - mae: 0.2078 - val_loss: 0.2526 - val_mae: 0.3801\n",
      "Epoch 17/20\n",
      "402/402 [==============================] - 101s 250ms/step - loss: 0.0840 - mae: 0.2074 - val_loss: 0.2270 - val_mae: 0.3612\n",
      "Epoch 18/20\n",
      "402/402 [==============================] - 108s 268ms/step - loss: 0.0841 - mae: 0.2073 - val_loss: 0.2243 - val_mae: 0.3633\n",
      "Epoch 19/20\n",
      "402/402 [==============================] - 111s 277ms/step - loss: 0.0830 - mae: 0.2059 - val_loss: 0.2392 - val_mae: 0.3737\n",
      "Epoch 20/20\n",
      "402/402 [==============================] - 113s 282ms/step - loss: 0.0831 - mae: 0.2059 - val_loss: 0.2425 - val_mae: 0.3741\n",
      "========================= Model 4/40 =========================\n",
      "| Starting with model | NF01_4_l-32_l-64_l-64_l-128_l-128 |\n",
      "| Starting time       | 2025-12-03 19:08:41.856273        |\n",
      "Epoch 1/20\n",
      "402/402 [==============================] - 137s 323ms/step - loss: 0.2726 - mae: 0.3896 - val_loss: 0.6881 - val_mae: 0.6705\n",
      "Epoch 2/20\n",
      "402/402 [==============================] - 147s 366ms/step - loss: 0.1625 - mae: 0.2922 - val_loss: 0.4903 - val_mae: 0.5350\n",
      "Epoch 3/20\n",
      "402/402 [==============================] - 144s 358ms/step - loss: 0.1243 - mae: 0.2500 - val_loss: 0.3991 - val_mae: 0.4797\n",
      "Epoch 4/20\n",
      "402/402 [==============================] - 154s 382ms/step - loss: 0.1016 - mae: 0.2279 - val_loss: 0.4066 - val_mae: 0.4918\n",
      "Epoch 5/20\n",
      "402/402 [==============================] - 151s 375ms/step - loss: 0.0909 - mae: 0.2156 - val_loss: 0.2937 - val_mae: 0.3960\n",
      "Epoch 6/20\n",
      "402/402 [==============================] - 149s 372ms/step - loss: 0.0880 - mae: 0.2124 - val_loss: 0.2729 - val_mae: 0.3759\n",
      "Epoch 7/20\n",
      "402/402 [==============================] - 147s 365ms/step - loss: 0.0873 - mae: 0.2116 - val_loss: 0.2686 - val_mae: 0.3717\n",
      "Epoch 8/20\n",
      "402/402 [==============================] - 151s 376ms/step - loss: 0.0876 - mae: 0.2123 - val_loss: 0.2513 - val_mae: 0.3591\n",
      "Epoch 9/20\n",
      "402/402 [==============================] - 156s 389ms/step - loss: 0.0877 - mae: 0.2126 - val_loss: 0.2271 - val_mae: 0.3424\n",
      "Epoch 10/20\n",
      "402/402 [==============================] - 155s 385ms/step - loss: 0.0872 - mae: 0.2120 - val_loss: 0.2441 - val_mae: 0.3528\n",
      "Epoch 11/20\n",
      "402/402 [==============================] - 154s 382ms/step - loss: 0.0868 - mae: 0.2114 - val_loss: 0.2707 - val_mae: 0.3688\n",
      "Epoch 12/20\n",
      "402/402 [==============================] - ETA: 0s - loss: 0.0864 - mae: 0.2110Restoring model weights from the end of the best epoch: 9.\n",
      "402/402 [==============================] - 150s 373ms/step - loss: 0.0864 - mae: 0.2110 - val_loss: 0.2814 - val_mae: 0.3750\n",
      "Epoch 12: early stopping\n",
      "========================= Model 5/40 =========================\n",
      "| Starting with model | NF01_5_l-32_l-64_l-64_l-128_l-128_d-0.2 |\n",
      "| Starting time       | 2025-12-03 19:38:37.593371              |\n",
      "Epoch 1/20\n",
      "403/403 [==============================] - 58s 123ms/step - loss: 0.2570 - mae: 0.3809 - val_loss: 0.3821 - val_mae: 0.4896\n",
      "Epoch 2/20\n",
      "403/403 [==============================] - 46s 114ms/step - loss: 0.1567 - mae: 0.2884 - val_loss: 0.3942 - val_mae: 0.4896\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 46s 115ms/step - loss: 0.1314 - mae: 0.2612 - val_loss: 0.4102 - val_mae: 0.5061\n",
      "Epoch 4/20\n",
      "403/403 [==============================] - ETA: 0s - loss: 0.1163 - mae: 0.2473Restoring model weights from the end of the best epoch: 1.\n",
      "403/403 [==============================] - 46s 114ms/step - loss: 0.1163 - mae: 0.2473 - val_loss: 0.4112 - val_mae: 0.5033\n",
      "Epoch 4: early stopping\n",
      "========================= Model 6/40 =========================\n",
      "| Starting with model | NF01_6_l-32_l-64_l-64_l-128_l-128_d-0.2 |\n",
      "| Starting time       | 2025-12-03 19:41:55.494369              |\n",
      "Epoch 1/20\n",
      " 89/403 [=====>........................] - ETA: 58s - loss: 0.4211 - mae: 0.5080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403/403 [==============================] - 82s 203ms/step - loss: 0.1669 - mae: 0.2985 - val_loss: 0.3967 - val_mae: 0.4823\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 79s 196ms/step - loss: 0.1363 - mae: 0.2656 - val_loss: 0.2916 - val_mae: 0.3987\n",
      "Epoch 4/20\n",
      "127/403 [========>.....................] - ETA: 46s - loss: 0.1464 - mae: 0.2665"
     ]
    }
   ],
   "source": [
    "# ===== 训练所有模型（多配置搜索） =====\n",
    "\n",
    "MAX_MODELS   = len(models)      # 想先试少一点可以改小\n",
    "EPOCHS       = EPOCHS\n",
    "PATIENCE     = PATIENCE\n",
    "MIN_DELTA    = MIN_DELTA\n",
    "SAVE_MODELS  = True\n",
    "FLUSH_EVERY  = 2\n",
    "\n",
    "epochs   = EPOCHS\n",
    "patience = PATIENCE\n",
    "min_delta = MIN_DELTA\n",
    "\n",
    "models = models[:MAX_MODELS]\n",
    "\n",
    "def pick(h, *keys):\n",
    "    for k in keys:\n",
    "        if k in h:\n",
    "            return h[k]\n",
    "    raise KeyError(f\"history 中没有 {keys}，可用键：{list(h.keys())}\")\n",
    "\n",
    "start_time   = t.time()\n",
    "pending_rows = []\n",
    "\n",
    "for idx, m in enumerate(models, 1):\n",
    "    stopper = t.time()\n",
    "    print('========================= Model {}/{} ========================='.format(idx, len(models)))\n",
    "    print(tabulate(\n",
    "        [['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]],\n",
    "        tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "    ))\n",
    "    try:\n",
    "        # 1) 构建模型\n",
    "        model = lstm.create_model(\n",
    "            layers=m['layers'],\n",
    "            timesteps=m['timesteps'],\n",
    "            features=X_train.shape[1]\n",
    "        )\n",
    "\n",
    "        # 2) 训练：用序列窗口 rearrange=True\n",
    "        history = lstm.train_model(\n",
    "            model=model,\n",
    "            mode='fit',\n",
    "            y=y_train.values,\n",
    "            X=X_train.values,\n",
    "            batch_size=m['batch_size'],\n",
    "            timesteps=m['timesteps'],\n",
    "            epochs=epochs,\n",
    "            rearrange=True,\n",
    "            validation_split=validation_split,\n",
    "            verbose=1,\n",
    "            early_stopping=early_stopping,\n",
    "            min_delta=min_delta,\n",
    "            patience=patience\n",
    "        )\n",
    "\n",
    "        h = history.history\n",
    "        val_loss_hist   = pick(h, 'val_loss')\n",
    "        train_loss_hist = pick(h, 'loss')\n",
    "        train_mae_hist  = pick(h, 'mae', 'mean_absolute_error')\n",
    "        val_mae_hist    = pick(h, 'val_mae', 'val_mean_absolute_error')\n",
    "\n",
    "        min_idx   = int(np.argmin(val_loss_hist))\n",
    "        min_epoch = min_idx + 1\n",
    "\n",
    "        row = {\n",
    "            'model_name': m['name'],\n",
    "            'config': m,\n",
    "            'dropout': m['layers'][0].get('dropout', 0),\n",
    "            'train_loss': float(train_loss_hist[min_idx]),\n",
    "            'train_rmse': 0,\n",
    "            'train_mae' : float(train_mae_hist[min_idx]),\n",
    "            'train_mape': 0,\n",
    "            'valid_loss': float(val_loss_hist[min_idx]),\n",
    "            'valid_rmse': 0,\n",
    "            'valid_mae' : float(val_mae_hist[min_idx]),\n",
    "            'valid_mape': 0,\n",
    "            'test_rmse': 0, 'test_mae': 0, 'test_mape': 0,\n",
    "            'epochs': f'{min_epoch}/{epochs}',\n",
    "            'batch_train': m['batch_size'],\n",
    "            'input_shape': (m['timesteps'], X_train.shape[1]),\n",
    "            'total_time': t.time() - stopper,\n",
    "            'time_step': m['timesteps'],\n",
    "            'splits': f\"0-{len(X_train)}\"\n",
    "        }\n",
    "        pending_rows.append(row)\n",
    "\n",
    "        if SAVE_MODELS:\n",
    "            model_path = os.path.join(model_dir, f\"{m['name']}.h5\")\n",
    "            model.save(model_path)\n",
    "\n",
    "        if (idx % FLUSH_EVERY == 0) or (idx == len(models)):\n",
    "            if pending_rows:\n",
    "                results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "                results.to_csv(output_table, sep=';', index=False)\n",
    "                pending_rows = []\n",
    "\n",
    "    except BaseException as e:\n",
    "        print('=============== ERROR {}/{} ============='.format(idx, len(models)))\n",
    "        print(tabulate(\n",
    "            [['Model:', m['name']], ['Config:', m]],\n",
    "            tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"\n",
    "        ))\n",
    "        print('Error:', e)\n",
    "        pending_rows.append({'model_name': m['name'], 'config': m, 'train_loss': str(e)})\n",
    "    finally:\n",
    "        try:\n",
    "            keras.backend.clear_session()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if pending_rows:\n",
    "    results = pd.concat([results, pd.DataFrame(pending_rows)], ignore_index=True)\n",
    "    results.to_csv(output_table, sep=';', index=False)\n",
    "\n",
    "print(\"Done. Results saved to:\", output_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f98be-ea0f-4ca9-b7ab-996f71a997af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 模型选择：按验证集 valid_mae 选 Top-5 =====\n",
    "import glob\n",
    "\n",
    "pattern    = os.path.join(res_dir, f\"{model_cat_id}_results_*.csv\")\n",
    "candidates = sorted(glob.glob(pattern))\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    results_csv = results.copy()\n",
    "    print(\"⚠ 没找到结果文件，使用当前内存里的 results\")\n",
    "else:\n",
    "    results_fn  = candidates[-1]\n",
    "    print(\"使用结果文件:\", results_fn)\n",
    "    results_csv = pd.read_csv(results_fn, delimiter=';')\n",
    "\n",
    "print(\"列名:\", list(results_csv.columns))\n",
    "\n",
    "if 'valid_mae' not in results_csv.columns:\n",
    "    raise ValueError(f\"'valid_mae' 列不存在，可用列: {list(results_csv.columns)}\")\n",
    "\n",
    "top_models = results_csv.dropna(subset=['valid_mae']).nsmallest(5, 'valid_mae')\n",
    "top_models = top_models.reset_index(drop=True)\n",
    "print(\"Top-5 models:\")\n",
    "display(top_models[['model_name', 'valid_mae', 'train_mae', 'epochs']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e549e7f-a548-4fea-a91c-200d3a2f7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "test_rows   = []\n",
    "predictions = {}\n",
    "\n",
    "for i, row in top_models.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    print(f\">>> [{i+1}/{len(top_models)}] Evaluating {model_name}\")\n",
    "\n",
    "    base = os.path.join(model_dir, model_name)\n",
    "    filename = base + \".keras\" if os.path.exists(base + \".keras\") else base + \".h5\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"  [WARN] 模型文件不存在:\", filename)\n",
    "        continue\n",
    "\n",
    "    model = load_model(filename)\n",
    "\n",
    "    batch_size_eval = int(row.get('batch_train', 64))\n",
    "    t_steps = int(row.get('time_step', TIMESTEPS))\n",
    "\n",
    "    # 用同一套序列化逻辑，确保长度一致\n",
    "    X_seq, y_seq = lstm.make_sequences(X_test.values, y_test.values, t_steps)\n",
    "\n",
    "    max_batch = len(X_seq) // batch_size_eval\n",
    "    X_eval = X_seq[:max_batch * batch_size_eval]\n",
    "    y_eval = y_seq[:max_batch * batch_size_eval]\n",
    "\n",
    "    print(\"  X_eval:\", X_eval.shape, \"y_eval:\", y_eval.shape)\n",
    "\n",
    "    pred = model.predict(X_eval, batch_size=batch_size_eval, verbose=0)\n",
    "    pred = pred.reshape(-1)\n",
    "    y_eval = y_eval.reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(y_eval, pred)\n",
    "    mse = mean_squared_error(y_eval, pred)\n",
    "\n",
    "    test_rows.append({\n",
    "        \"Model name\": model_name,\n",
    "        \"Mean squared error\": mse,\n",
    "        \"Mean absolute error\": mae\n",
    "    })\n",
    "\n",
    "    predictions[model_name] = {\n",
    "        \"pred_scaled\": pred.copy(),\n",
    "        \"y_scaled\": y_eval.copy()\n",
    "    }\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    del model\n",
    "\n",
    "test_results = pd.DataFrame(test_rows).sort_values(\"Mean absolute error\")\n",
    "test_results = test_results.set_index(\"Model name\")\n",
    "print(\"\\nTest performance:\")\n",
    "print(tabulate(test_results, headers=\"keys\", tablefmt=\"grid\", numalign=\"right\", floatfmt=\".4f\"))\n",
    "\n",
    "test_results.to_csv(test_output_table, sep=';', index=True)\n",
    "print(\"Saved test results to:\", test_output_table)\n",
    "\n",
    "best_model_name = test_results.index[0]\n",
    "print(\"\\nBest model on test set:\", best_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147d29b-14db-441a-8efd-56937165d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 最佳模型预测 & 异常检测 =====\n",
    "best_model_file = os.path.join(model_dir, best_model_name + \".h5\")\n",
    "best_model = load_model(best_model_file)\n",
    "\n",
    "batch_size_best = int(top_models.loc[top_models['model_name'] == best_model_name, 'batch_train'].iloc[0])\n",
    "t_steps_best    = int(top_models.loc[top_models['model_name'] == best_model_name, 'time_step'].iloc[0])\n",
    "\n",
    "# 序列化测试集\n",
    "X_seq, y_seq = lstm.make_sequences(X_test.values, y_test.values, t_steps_best)\n",
    "max_batch = len(X_seq) // batch_size_best\n",
    "X_eval = X_seq[:max_batch * batch_size_best]\n",
    "y_eval = y_seq[:max_batch * batch_size_best]\n",
    "\n",
    "pred_scaled = best_model.predict(X_eval, batch_size=batch_size_best, verbose=0).reshape(-1)\n",
    "y_scaled    = y_eval.reshape(-1)\n",
    "\n",
    "# 反归一化回原始单位\n",
    "y_true = scaler_y.inverse_transform(y_scaled.reshape(-1,1)).reshape(-1)\n",
    "y_pred = scaler_y.inverse_transform(pred_scaled.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "# 计算残差\n",
    "residuals = np.abs(y_true - y_pred)\n",
    "\n",
    "print(\"原始尺度 MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "print(\"原始尺度 MSE:\", mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# ===== 简单异常检测：按残差做阈值 =====\n",
    "# 用均值 + 3*std 作为阈值，或者 99 分位数\n",
    "thr_mean_std = residuals.mean() + 3 * residuals.std()\n",
    "thr_quantile = np.quantile(residuals, 0.99)\n",
    "\n",
    "print(\"Threshold (mean+3std):\", thr_mean_std)\n",
    "print(\"Threshold (99% quantile):\", thr_quantile)\n",
    "\n",
    "# 选一个你喜欢的阈值（这里用 quantile）\n",
    "threshold = thr_quantile\n",
    "\n",
    "anomaly_flag = residuals > threshold\n",
    "\n",
    "# 对齐到原始索引：序列化会从第 t_steps 开始\n",
    "idx_eval = y_test.index[t_steps_best : t_steps_best + len(y_true)]\n",
    "\n",
    "anomaly_df = pd.DataFrame({\n",
    "    \"minute_index\": idx_eval,\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"residual\": residuals,\n",
    "    \"is_anomaly\": anomaly_flag.astype(int)\n",
    "})\n",
    "anomaly_df = anomaly_df.set_index(\"minute_index\")\n",
    "\n",
    "anomaly_path = os.path.join(\n",
    "    res_dir, f\"{best_model_name}_anomalies_{t.strftime('%Y%m%d')}.csv\"\n",
    ")\n",
    "anomaly_df.to_csv(anomaly_path, sep=';')\n",
    "print(\"Anomalies saved to:\", anomaly_path)\n",
    "\n",
    "# 简单画个图看看\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(idx_eval, y_true, label=\"actual\")\n",
    "plt.plot(idx_eval, y_pred, label=\"pred\")\n",
    "plt.scatter(idx_eval[anomaly_flag], y_true[anomaly_flag], marker='x', label=\"anomaly\")\n",
    "plt.legend()\n",
    "plt.title(\"Best model prediction & anomalies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6deb97d-b646-4482-ae9c-875b02fd9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"开始回放测试集，模拟在线预警（按时间顺序）...\\n\")\n",
    "print(\"格式：时间索引 actual  pred  residual   是否告警\")\n",
    "\n",
    "for i, idx in enumerate(idx_eval):\n",
    "    real = y_true[i]\n",
    "    pred = y_pred[i]\n",
    "    res  = residuals[i]\n",
    "    is_anom = res > threshold  # 用你前面算的阈值（比如 99% 分位数）\n",
    "\n",
    "    flag = \"  <-- ⚠ ALERT 异常流量\" if is_anom else \"\"\n",
    "    print(f\"{idx:6d}  {real:8.0f}  {pred:8.0f}  {res:8.0f}{flag}\")\n",
    "\n",
    "    # 为了演示效果慢一点，可以调节速度\n",
    "    time.sleep(0.05)   # 50ms 一条，课堂上看着比较“流动”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7102e-2b00-4b10-8ad8-aab7b452f6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (network_proj_new)",
   "language": "python",
   "name": "network_proj_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
